library(lda)
library(stringr)
library(tm)
library(topicmodels)
library(LDAvis)
library(servr)
library(LDAvisData)
library(MASS)
# 모델 트레이닝
PimaCV.lda <- lda(type ~ ., data = Pima.tr, CV = TRUE)
tab <- table(Pima.tr$type, PimaCV.lda$class)
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
dimnames(conCV1) <- list(Actual = c("No", "Yes"), "Predicted (cv)" = c("No", "Yes"))
print(round(conCV1, 3))
Pima.lda <- lda(type ~ ., data = Pima.tr)
Pima.hat <- predict(Pima.lda)
tabtrain <- table(Pima.tr$type, Pima.hat$class)
# 텍스트 파일의 모든 행 읽어오기
data = readLines("test_lda_wordset", encoding="UTF-8")
doc.list <- strsplit(reviews, "[[:space:]]+") # 공백을 기준으로 문장 분리
doc.list <- doc.list[lengths(doc.list) >0]    # 빈 줄 삭제
# 단어 빈도 계산 후 테이블로 표현
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)
# LDA 패키지에서 사용할 수 있는 형태로 변환
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# 데이터 셋에 관련된 수치들 계산
D <- length(documents)  # 문서의 갯수
W <- length(vocab)  # 단어의 갯수
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # 문서당 토큰의 갯수 [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # 데이터 전체의 토큰 갯수 (546,827)
term.frequency <- as.integer(term.table)  # 형태소 출현 빈도 [8939, 5544, 2411, 2410, 2143, ...]
# MCMC and model tuning parameters:
K <- 5
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
setwd('/Users/throughkim/Documents/py_projects/multicore_korean_analyzer')
library(lda)
library(stringr)
library(tm)
library(topicmodels)
library(LDAvis)
library(servr)
library(LDAvisData)
library(MASS)
# 모델 트레이닝
PimaCV.lda <- lda(type ~ ., data = Pima.tr, CV = TRUE)
tab <- table(Pima.tr$type, PimaCV.lda$class)
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
dimnames(conCV1) <- list(Actual = c("No", "Yes"), "Predicted (cv)" = c("No", "Yes"))
print(round(conCV1, 3))
Pima.lda <- lda(type ~ ., data = Pima.tr)
Pima.hat <- predict(Pima.lda)
tabtrain <- table(Pima.tr$type, Pima.hat$class)
# 텍스트 파일의 모든 행 읽어오기
data = readLines("test_lda_wordset", encoding="UTF-8")
doc.list <- strsplit(reviews, "[[:space:]]+") # 공백을 기준으로 문장 분리
doc.list <- doc.list[lengths(doc.list) >0]    # 빈 줄 삭제
# 단어 빈도 계산 후 테이블로 표현
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)
# LDA 패키지에서 사용할 수 있는 형태로 변환
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# 데이터 셋에 관련된 수치들 계산
D <- length(documents)  # 문서의 갯수
W <- length(vocab)  # 단어의 갯수
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # 문서당 토큰의 갯수 [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # 데이터 전체의 토큰 갯수 (546,827)
term.frequency <- as.integer(term.table)  # 형태소 출현 빈도 [8939, 5544, 2411, 2410, 2143, ...]
# MCMC and model tuning parameters:
K <- 5
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
library(lda)
library(stringr)
library(tm)
library(topicmodels)
library(LDAvis)
library(servr)
library(LDAvisData)
library(MASS)
# 모델 트레이닝
PimaCV.lda <- lda(type ~ ., data = Pima.tr, CV = TRUE)
tab <- table(Pima.tr$type, PimaCV.lda$class)
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
dimnames(conCV1) <- list(Actual = c("No", "Yes"), "Predicted (cv)" = c("No", "Yes"))
print(round(conCV1, 3))
Pima.lda <- lda(type ~ ., data = Pima.tr)
Pima.hat <- predict(Pima.lda)
tabtrain <- table(Pima.tr$type, Pima.hat$class)
# 텍스트 파일의 모든 행 읽어오기
data = readLines("test_lda_wordset", encoding="UTF-8")
doc.list <- strsplit(reviews, "[[:space:]]+") # 공백을 기준으로 문장 분리
doc.list <- doc.list[lengths(doc.list) >0]    # 빈 줄 삭제
# 단어 빈도 계산 후 테이블로 표현
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)
# LDA 패키지에서 사용할 수 있는 형태로 변환
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# 데이터 셋에 관련된 수치들 계산
D <- length(documents)  # 문서의 갯수
W <- length(vocab)  # 단어의 갯수
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # 문서당 토큰의 갯수 [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # 데이터 전체의 토큰 갯수 (546,827)
term.frequency <- as.integer(term.table)  # 형태소 출현 빈도 [8939, 5544, 2411, 2410, 2143, ...]
# MCMC and model tuning parameters:
K <- 5
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
library(lda)
library(stringr)
library(tm)
library(topicmodels)
library(LDAvis)
library(servr)
library(LDAvisData)
library(MASS)
# 모델 트레이닝
PimaCV.lda <- lda(type ~ ., data = Pima.tr, CV = TRUE)
tab <- table(Pima.tr$type, PimaCV.lda$class)
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
dimnames(conCV1) <- list(Actual = c("No", "Yes"), "Predicted (cv)" = c("No", "Yes"))
print(round(conCV1, 3))
Pima.lda <- lda(type ~ ., data = Pima.tr)
Pima.hat <- predict(Pima.lda)
tabtrain <- table(Pima.tr$type, Pima.hat$class)
# 텍스트 파일의 모든 행 읽어오기
text_data = readLines("total_output", encoding="UTF-8") ## 텍스트 파일의 모든 행 읽어오기
doc.list <- strsplit(text_data, "[[:space:]]+") # 공백을 기준으로 문장 분리
doc.list <- doc.list[lengths(doc.list) >0]    # 빈 줄 삭제
# 단어 빈도 계산 후 테이블로 표현
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)
# LDA 패키지에서 사용할 수 있는 형태로 변환
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# 데이터 셋에 관련된 수치들 계산
D <- length(documents)  # 문서의 갯수
W <- length(vocab)  # 단어의 갯수
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # 문서당 토큰의 갯수 [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # 데이터 전체의 토큰 갯수 (546,827)
term.frequency <- as.integer(term.table)  # 형태소 출현 빈도 [8939, 5544, 2411, 2410, 2143, ...]
# MCMC and model tuning parameters:
K <- 5
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
library(lda)
library(stringr)
library(tm)
library(topicmodels)
library(LDAvis)
library(servr)
library(LDAvisData)
library(MASS)
# 모델 트레이닝
PimaCV.lda <- lda(type ~ ., data = Pima.tr, CV = TRUE)
tab <- table(Pima.tr$type, PimaCV.lda$class)
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
dimnames(conCV1) <- list(Actual = c("No", "Yes"), "Predicted (cv)" = c("No", "Yes"))
print(round(conCV1, 3))
Pima.lda <- lda(type ~ ., data = Pima.tr)
Pima.hat <- predict(Pima.lda)
tabtrain <- table(Pima.tr$type, Pima.hat$class)
# 텍스트 파일의 모든 행 읽어오기
text_data = readLines("total_output", encoding="UTF-8") ## 텍스트 파일의 모든 행 읽어오기
doc.list <- strsplit(text_data, "[[:space:]]+") # 공백을 기준으로 문장 분리
doc.list <- doc.list[lengths(doc.list) >0]    # 빈 줄 삭제
# 단어 빈도 계산 후 테이블로 표현
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)
vocab
# LDA 패키지에서 사용할 수 있는 형태로 변환
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
documents
# 데이터 셋에 관련된 수치들 계산
D <- length(documents)  # 문서의 갯수
W <- length(vocab)  # 단어의 갯수
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # 문서당 토큰의 갯수 [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # 데이터 전체의 토큰 갯수 (546,827)
term.frequency <- as.integer(term.table)  # 형태소 출현 빈도 [8939, 5544, 2411, 2410, 2143, ...]
# 데이터 셋에 관련된 수치들 계산
D <- length(documents)  # 문서의 갯수
W <- length(vocab)  # 단어의 갯수
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # 문서당 토큰의 갯수 [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # 데이터 전체의 토큰 갯수 (546,827)
term.frequency <- as.integer(term.table)  # 형태소 출현 빈도 [8939, 5544, 2411, 2410, 2143, ...]
# MCMC and model tuning parameters:
K <- 5
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
library(lda)
library(stringr)
library(tm)
library(topicmodels)
library(LDAvis)
library(servr)
library(LDAvisData)
library(MASS)
# 모델 트레이닝
PimaCV.lda <- lda(type ~ ., data = Pima.tr, CV = TRUE)
tab <- table(Pima.tr$type, PimaCV.lda$class)
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
dimnames(conCV1) <- list(Actual = c("No", "Yes"), "Predicted (cv)" = c("No", "Yes"))
print(round(conCV1, 3))
Pima.lda <- lda(type ~ ., data = Pima.tr)
Pima.hat <- predict(Pima.lda)
tabtrain <- table(Pima.tr$type, Pima.hat$class)
# 텍스트 파일의 모든 행 읽어오기
text_data = readLines("total_output", encoding="UTF-8") ## 텍스트 파일의 모든 행 읽어오기
doc.list <- strsplit(text_data, "[[:space:]]+") # 공백을 기준으로 문장 분리
doc.list <- doc.list[lengths(doc.list) >0]    # 빈 줄 삭제
# 단어 빈도 계산 후 테이블로 표현
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)
# LDA 패키지에서 사용할 수 있는 형태로 변환
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# 데이터 셋에 관련된 수치들 계산
D <- length(documents)  # 문서의 갯수
W <- length(vocab)  # 단어의 갯수
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # 문서당 토큰의 갯수 [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # 데이터 전체의 토큰 갯수 (546,827)
term.frequency <- as.integer(term.table)  # 형태소 출현 빈도 [8939, 5544, 2411, 2410, 2143, ...]
# MCMC and model tuning parameters:
K <- 5
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
library(lda)
library(stringr)
library(tm)
library(topicmodels)
library(LDAvis)
library(servr)
library(LDAvisData)
library(MASS)
PimaCV.lda <- lda(type ~ ., data = Pima.tr, CV = TRUE)
tab <- table(Pima.tr$type, PimaCV.lda$class)
conCV1 <- rbind(tab[1, ]/sum(tab[1, ]), tab[2, ]/sum(tab[2, ]))
dimnames(conCV1) <- list(Actual = c("No", "Yes"), "Predicted (cv)" = c("No", "Yes"))
print(round(conCV1, 3))
Pima.lda <- lda(type ~ ., data = Pima.tr)
Pima.hat <- predict(Pima.lda)
tabtrain <- table(Pima.tr$type, Pima.hat$class)
text_data = readLines("total_lda_wordset", encoding="UTF-8") ## 텍스트 파일의 모든 행 읽어오기
reviews<-text_data
data<-reviews
# read in some stopwords:
library(tm)
stop_words <- stopwords("SMART")
doc.list <- strsplit(reviews, "[[:space:]]+") # 공백을 기준으로 문장 분리
doc.list <- doc.list[lengths(doc.list) >0]    # 빈 줄 삭제
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
# remove terms that are stop words or occur fewer than 3 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (2,000)
W <- length(vocab)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]
# MCMC and model tuning parameters:
K <- 5
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
